{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Chicken or the Beef? Why Everyone Loves Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Ra Inta*\n",
    "\n",
    "*MIT licence, 2019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "You're probably here because you've heard how awesome Artificial Neural Networks (ANNs) or Deep Learning algorithms are, and would like to know a few more details. Welcome! Make yourself at home.\n",
    "\n",
    "We'll take you on a brief tour of the conceptual foundations behind ANNs, through the lens of having to make a meal choice whilst stuck on a long plane flight. We'll see how to encode this problem using Python's Keras and TensorFlow libraries, before extending the model to more complex cases, by constructing ANNs with various architectures. This will lead the way -- at least provide a sketch of the intricate map -- towards Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable inline plotting for graphics\n",
    "%matplotlib inline\n",
    "## Output more than the last command in a cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the versions of the libraries I will be using for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.7 | packaged by conda-forge | (default, Feb 26 2019, 03:50:56) \n",
      "[GCC 7.3.0]\n",
      "Pandas version: 0.24.1\n",
      "Matplotlib version: 3.0.3\n",
      "Numpy version: 1.15.4\n",
      "Tensorflow version: 1.10.0\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)\n",
    "print(\"Pandas version: {0}\".format(pd.__version__))\n",
    "print(\"Matplotlib version: {0}\".format(matplotlib.__version__))\n",
    "print(\"Numpy version: {0}\".format(np.__version__))\n",
    "print(\"Tensorflow version: {0}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most basic artificial neuron: the Perceptron\n",
    "\n",
    "Let's say you have an important decision to make. You are hungry and wish to be sated. However, you are on an airplane, so your options are 'chicken' or 'beef'. Your target variable is 'satisfaction,' which will not be achieved if you choose neither, or both, options. You must make this decision while you have the flight attendant's attention. Finally, the attendant is new and very busy, and may not recall your first choice, so you will have to be explicit about what you _do not_ wish, as well as what you do (or perhaps they ran out of one option and another attendant has to provide this option later, without information on the first meal option).\n",
    "\n",
    "This contrived scenario is simple by design. Yet how would we go about modeling this decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could approach this as a linear regression problem: $f(x) = w_1x_1 + w_2x_2 + b$, where the $w_i$ are the weights of the inputs $x_i$, and minimize the least squares cost function, $J(x) = \\sum_{r} (f'(x) - f(x))^2$, where the $r$ run over all the input values.\n",
    "\n",
    "However, consider an intrinsic problem here. The final decision depends wholly on the interaction between the two input variables, 'chicken' (say, $x_1$) or 'beef' ($x_2$):\n",
    "\n",
    "![Decision plane for meals... on a plane](images/chicken_or_beef_decision_plane.png)\n",
    "\n",
    "If $x_1$ is maximal, we wish the value of $x_2$ to be minimal, and vice-versa. But we can't just change the coefficients from a regression at will!\n",
    "\n",
    "In other words, there is no single line that separates the Chicken-Beef ($x_1-x_2$) plane to define a distinct _decision boundary_ between the two classes, 'sated' and 'not sated'. However, we may do this with _two_ lines (and logical comparisons):\n",
    "\n",
    "![Division of decision plane using two linear regions](images/chicken_or_beef_twoLines2.png)\n",
    "\n",
    "The lower (green) line marks a boundary between the (0, 0) point and the other three, containing all but the origin point. The upper (blue) line defines a region that contains only the (1, 1) point.\n",
    "\n",
    "In other words, the regions defined by:\n",
    "\n",
    "$x_2 \\geq -x_1 + 0.5$\n",
    "\n",
    "$x_2 \\geq -x_1 + 1.2$\n",
    "\n",
    "each return 1 if the condition is met (note that we had a _lot_ of flexibility here; we just had to choose two lines that separated the two classes. I just chose those with slopes of -1 each, for simplicity). This may be re-cast:\n",
    "\n",
    "$x_1 + x_2 \\geq 0.5 \\rightarrow H(x_1 + x_2 - 0.5)$\n",
    "\n",
    "$x_1 + x_2 \\geq 1.2 \\rightarrow H(x_1 + x_2 - 1.2)$\n",
    "\n",
    "The simplest function to return a 1 or 0 depending on a fixed criterion, is the Heaviside step-function, $H(x)$, with the following properties:\n",
    "$H(x) = \\{ ^{1\\textrm{ for }x > 0} _{0\\textrm{ for x } \\leq 0}$\n",
    "\n",
    "A schematic allowing these two lines to interact with each other, for this comparison may look like:\n",
    "\n",
    "![Schematic of algorithm determining membership of classification regions ('sated' and 'not sated')](images/Simple_MLP_figure.png)\n",
    "\n",
    "Where the last calculation is a simple logical comparison (`and`) of the regions.\n",
    "\n",
    "This, which is really the construction of the XOR function, is the essential idea behind the most fundamental neural net: the perceptron. \n",
    "\n",
    "We have just seen that, while a trusty linear regression failed to model our meal satisfaction function, a collection of simple units that can easily interact and compare with each other, performed the task quite nicely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks (ANNs)\n",
    "\n",
    "Artificial Neural Networks (ANNs; often further abbreviated to NNs) are inspired directly from our understanding of brain neurophysiology. An individual _neuron_ is the basic cell unit of our complex central nervous system. Each neuron takes inputs, in the form of electrical signals, and performs a number of simple transforms on these inputs, resulting in a simple output. These outputs are in turn fed as inputs to other neurons.\n",
    "\n",
    "Artificial neurons are simplified analogs of these biological units, taking in a limited number of signals, performing simple operations on them, before emitting a limited number of output signals. The astounding computational capabilities of this class of algorithm arise from the networks built up using these simple units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of a Neural Network \n",
    "\n",
    "An ANN is composed of _neurons_ (_nodes_) and _layers_. Each node performs the atomic operations of the network, defined by _activation functions_. Groups of nodes may form a layer, a distinct structure representing a stage of the network. Each layer acts like a filter, or function. At least two layers are defined: the _input layer_ and the _output layer_. In addition, there may be one or more layer that is neither an input nor an output; these are referred to as _hidden layers_:\n",
    "\n",
    "\n",
    "\n",
    "![Achitecture of a typical Artificial Neural Network (ANN). The number of layers determine the _depth_ of the model; the number of neurons in each layer determine the _width_ of the model.](images/ANN_architecture_intro.png)\n",
    "\n",
    "The purpose of ANNs is to approximate any arbitrary function, say, $f'(x)$. Each layer can be thought of as a successive function $f_i()$ acting on the previous layers. The particular composition of layers and nodes of a neural network is known as the _net architecture_.\n",
    "\n",
    "In this framework, for the chicken-beef calculation we performed above, each linear comparison was performed within a node (neuron), after being fed inputs $x_1$ and $x_2$. The outputs were fed into the final, output, node. This architecture is a Multi-Layer Perceptron (MLP). It is a particular type of _feed-forward network_, because there are no layers that make use of feedback.\n",
    "\n",
    "The activation function we chose (fairly organically!) was the Heaviside step function, $H(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "The purpose of an activation function is to polarize the network (_i.e._ provide directionality), as well as condition the signals propagated throughout (very often regulated to have a limited output range). The most common activation functions are:\n",
    "\n",
    " *  **Perceptron** (Heaviside): $\\sigma(z) = \\{ ^{1\\textrm{ for }z > 0} _{0\\textrm{ for z } \\leq 0}$\n",
    "\n",
    " *  **Sigmoid** (logistic): $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$\n",
    " \n",
    " * **ReLU** (Rectified Linear Unit): $\\sigma(z) = \\max{(0, z)}$\n",
    "\n",
    " *  **Softmax**: $\\sigma(z)_j = \\frac{\\exp{(z_j)}}{\\sum_{k}^{K} \\exp{(z_k)}}$\n",
    " \n",
    "Their response functions look like the following:\n",
    "\n",
    "![Response functions of three of the most popular activation functions for neurons in ANNs.](images/activation_function_comparison.png)\n",
    "\n",
    "The sigmoid (or logistic) activation function is a smoothed version of the step function, so has nicer analytic properties than the step function. However, it can be somewhat computationally expensive for large numbers of nodes and layers. The ReLU (Rectified Linear Unit) is a simpler function. Although, being piece-wise linear, it is still technically non-linear, it provides network polarity while retaining many properties of linearity which make these nice for approximating functions. ReLU-based neurons are much 'faster' to train because of their computational simplicity.\n",
    "\n",
    "The softmax activation function is an ensemble function, often used for an aggregation step. It also has nice analytic properties, regulating the output based on the ensemble mean. This often favors a 'winner-takes-all' condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So much for the theory. How do we code these things?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Keras and Tensorflow\n",
    "\n",
    "\n",
    "**Tensorflow**\n",
    "\n",
    "TensorFlow was developed by the Google Brain team, released to the Apache foundation in late 2015. It is a symbolic, high-performance, math library with specialized and generalized math objects, particularly _tensors_, a generalization of vector arithmetic and calculus (hence the name). The mental model for TensorFlow computations is a computational graph, defined by tensors. It is designed to be seamlessly applied to a range of hardware types (including GPGPUs and a specialized ASIC, the TPU---_Tensor Processing Unit_).  \n",
    "\n",
    "**Tensorflow documentation:** https://www.tensorflow.org/\n",
    "\n",
    "---\n",
    "**Keras**\n",
    "\n",
    "Keras is a high-level API to the neural network libraries CNTK, Theano and TensorFlow. Its high level of abstraction allows rapid prototyping of neural networks, with both convolution and recurrent network architectures. Its guiding principles are user-friendliness, modularity and to be easily extendible. Because it's written in Python, configuration and extension of functionality are relatively seamless within the Python eco-system. \n",
    "\n",
    "Keras is Greek for 'horn,' a reference to the vision-inducing spirits in the _Odyssey_.\n",
    "\n",
    "**Keras documentation:** https://keras.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and TensorFlow\n",
    "\n",
    "The fundamental unit of computation within TensorFlow is the _tensor_. Tensors are generalizations of vector arithmetic and calculus, allowing linear operations on higher rank objects. These are used to partially define a computation, in the form of a data-flow graph, that will, when executed, produce an output value. TensorFlow constructs a graph based on tensor objects (`tf.Tensor`). This graph is then executed within a TensorFlow session (`tf.Session()`) instance.\n",
    "\n",
    "We can simply generate a tensor object using `tf.Variable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "odd_nums = tf.Variable([1, 3, 5, 7, 9, 11])  # Rank 1 tensor is a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has the usual `.dtype` and `.shape` attributes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.int32_ref"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(6)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_nums.dtype\n",
    "odd_nums.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weird_hypercube = tf.Variable([ [ [43], [121.234] ], [ [987], [2134] ] ], dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as mentioned, tensors in TensorFlow are _partially_ computed graph objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Rank_2:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank1 = tf.rank(odd_nums)\n",
    "rank1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Rank_3:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank2 = tf.rank(weird_hypercube)\n",
    "rank2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_3:0' shape=(2, 2, 1) dtype=float64_ref>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weird_hypercube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations are not run until we have specified that our computation graph is complete. Note that the `tf.rank()` operations did not return an actual rank value. To execute operations on the tensors we just produced, we require a `tf.Session()` connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(rank1)\n",
    "    sess.run(rank2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the details of your computation this way, and generating the necessary graphs and sessions, requires quite some thought and a lot of boiler-plate code (of which is not particularly Pythonic!). \n",
    "\n",
    "There are so many commonly used operations and architectures, that a higher level API would be very useful! This is where Keras comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras and Tensorflow to Solve the Chicken-or-Beef Problem\n",
    "\n",
    "As solving programming puzzles go, this is like smashing a walnut with a hydraulic press:\n",
    "\n",
    "\n",
    "![Solving the chicken-or-beef problem with an ANN is like smashing a walnut with a hydraulic press -- or, in this case, a Bonsack machine. Original machine image in public domain; U.S. pat. #238,640](images/Walnut_press_labelled_smaller2.jpg)\n",
    "\n",
    "![Much smaller text in description to test image](images/good-good-let-the-power-of-pandas-flow-through-you.jpg)\n",
    "\n",
    "In fact, you'll see below that this problem is almost _too_ simple for our ANN coding framework (this occurs surprisingly often 'in the wild' at the moment too!). You will, however, hopefully see that setting up very sophisticated network architectures is made very easy using Keras. However, we’re already attached to this airplane meal puzzle, so we’ll see it through to its nutritious end.\n",
    "\n",
    "We wish to construct a binary classifier (binary because we only have two outputs: ‘sated’ and ‘not sated’) based on the conceptual MLP model we considered above. A tabular expression of our puzzle looks like:\n",
    "\n",
    "| Chicken | Beef | Sated |\n",
    "|---------|------|-------|\n",
    "| No | No | No |\n",
    "| Yes | No | Yes |\n",
    "| No | Yes | Yes |\n",
    "| Yes | Yes | No |\n",
    "\n",
    "We can encode this easily enough in Python. We’ll take our inputs, x, as a matrix, and the desired outcome, y, as a vector. In the Python eco-system, these are represented as Numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unit = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_unit = np.array([0, 1, 1, 0])\n",
    "x = x_unit\n",
    "y = y_unit\n",
    "\n",
    "for __ in range(6000):\n",
    "    x = np.vstack([x, x_unit])\n",
    "    y = np.hstack([y, y_unit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have replicated the input/output pairs 6,000 times. This is an unfortunate side-effect of the simplicity of the problem at hand. As I mentioned above, it's almost _too_ simple for the usual use-cases of this framework. \n",
    "\n",
    "How will we know our model is performing accurately, and we’re not sitting in a circle of yes-attendants, telling us our predictions are correct? We need to hold out some portion of the data (a validation data-set) to test our model against:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = x.shape[0]\n",
    "split_Idx = np.random.choice(range(data_len), int(0.75*data_len), replace=False)\n",
    "\n",
    "x_train = x[split_Idx]\n",
    "x_test = x[~np.in1d(range(data_len), split_Idx)]  # Note the bit-wise logical negation\n",
    "y_train = y[split_Idx]\n",
    "y_test = y[~np.in1d(range(data_len), split_Idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18003, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(18003,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(6001, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(6001,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n",
    "y_train.shape\n",
    "x_test.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is very simply done if we added an import from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below if you already have sklearn installed\n",
    "# from sklearn.model_selection import train_test_split \n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, set some constants for re-use\n",
    "batch_size = 128 \n",
    "num_classes = 2  # Binary classifier\n",
    "num_epochs = 512  # Number of rounds of training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some additional imports\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has two types of model: `Sequential` and the functional `Model`. \n",
    "Each model type shares the following attributes:\n",
    "\n",
    "* `.layers`: the layers of the model\n",
    "* `.inputs`: the input tensors of the model\n",
    "* `.outputs`: the output tensors\n",
    "\n",
    "They also have a `.summary()` method, giving a summary of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to call our first model! Instantiate an instance of the `Sequential()` model class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the initial (input) layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.add(Dense(64, activation='relu', input_shape=(2,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the final (output) layer:                                                                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.add(Dense(num_classes, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                192       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 322\n",
      "Trainable params: 322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have thus defined our model's architecture. We must now define _how_ the model determines it has found a suitable approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost and loss functions\n",
    "\n",
    "The important objective functions for ANNs are referred to as _cost_ or _loss_ functions. These functional measures of error are the important metrics with which we determine the success of our algorithm. The goal of machine learning algorithms is to optimize such a functions. The former term (_cost function_) is often reserved for the entire training set, in which case the _loss function_ is defined as the loss per epoch. \n",
    "\n",
    "We are already familiar with one of the most common cost functions, namely the Mean Squared Error (MSE). This is the routine used to obtain an Ordinary Least Squares (OLS) linear regression fit. Explicitly, we minimize the function:\n",
    "\n",
    "$J(x) = \\frac{1}{2n}\\sum{}^{}_{j} [f'_j(x) - f_j(x)]^2$\n",
    "\n",
    "Where $f'(x)$ denotes the function to be approximated and $f(x)$ represents the functional form used by the algorithm (here, the activation function used in each layer). Here, the sum is taken over each node, labeled with $j$. \n",
    "\n",
    "Another cost function, popularly used for binary classification, is the _cross-entropy_ (or Bernoulli negative log-likelihood or Binary Cross-Entropy):\n",
    "\n",
    "$J(x) = -\\sum{}_{j}[f(x)\\log_e{(f'(x))} + (1 - f(x))\\log_e{(1 - f'(x))}]$\n",
    "\n",
    "One problem with using the MSE cost function is that it can be very slow to learn with large errors for a sigmoid activation function. This is not the case for cross-entropy, hence it is widely used.\n",
    "\n",
    "There are a number of other cost functions, such as the Mean Absolute Error (MAE):\n",
    "\n",
    "$J(x) = \\frac{1}{n}\\sum{}^{}_{j} |f'_j(x) - f_j(x)|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "In practice, we do not usually have access to the analytic form of the cost or loss functions, and hence do not have an explicit expression for the optimal parameter values. We have to then rely on optimization schemes. Probably the best known are the Newton-Raphson family of optimization functions, which 'descend' to the optimal point, based on the gradient (this is known as _gradient descent_). \n",
    "\n",
    "Specifically, perhaps the most widely used optimizers for ANNs are based on _Stochastic Gradient Descent_ (SGD). Consider that the goal here is to minimize the cost function, $J(x)$. In other words, where $\\nabla J(x) = 0$ \n",
    "\n",
    "(Note that here we have adopted the gradient operator,$\\nabla$ ('nabla'); although we have written the above as a function of a single variable $\\nabla f(x) := \\partial f(x)/\\partial x$, $\\nabla$ is the derivative across all variables in the space.)  \n",
    "\n",
    "As an iterative process, we update the weights $w^{(r)}$, beginning from the initial $w^{(0)}$. We wish to find $\\nabla J(x)$; this may be well approximated by taking a random sample of training inputs and computing the (discrete) gradient by taking a group of random nodes (a _mini-batch_), $j \\in m$. In other words, we assume $\\nabla J(x) \\approx \\nabla J_m(x)$. \n",
    "\n",
    "The $r$th iteration of a weight is hence updated according to: $w^{(r)} = w^{(r-1)} - \\eta\\nabla J_m(w)$. The constant parameter, $\\eta > 0 $, is known as the _Learning Rate_, as it determines the rate at which the weights are updated.\n",
    "\n",
    "To help with numerical stability, the concept of _momentum_ has been introduced. This reduces oscillations, and overshoot of the global minimum, by introducing a term proportional to the incremental change in rate. Another useful parameter within enhancements to SGD optimization is the _decay_ parameter. This reduces the learning rate if the loss does not decrease after a set number of epochs. \n",
    "\n",
    "Even more sophisticated variants of SGD involve the automatic update of learning rates depending on how important a particular feature parameter is (Adagrad, Adadelta). We will use here the optimizer recommended as a great all-purpose default, the `RMSProp()` variant of Adadelta. In effect, this means that we do not have to attempt to tune the learning rate; this is done so automatically. \n",
    "\n",
    "For more details on variants of optimizers used in ANNs, I recommend Sebastian Ruder's _An overview of gradient descent optimization algorithms_:\n",
    " \n",
    "http://ruder.io/optimizing-gradient-descent/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of calling a Stochastic Gradient Descent (SGD) optimizer\n",
    "# keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking model performance: metrics\n",
    "\n",
    "It is vital to quantify how our models perform. Keras makes it simple to track a number of off-the-shelf loss functions, that are not used to update or train the model, but may elucidate its behavior. This may range from the simple `accuracy` (the mean difference between prediction and actual 'ground truth' values), mean absolute error (`mae`) or `categorical_accuracy`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within Keras, it is a simple matter to define the loss and optimizer functions, and performance metric to track for our MLP model. These are specified at the `compile` stage of the computation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "history0 = model0.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.1920928955078125e-07,      Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "score0 = model0.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {0},      Test accuracy: {1}'.format(score0[0], score0[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained a model on an arbitrary function, with an apparent accuracy of 100%!\n",
    "\n",
    "However, this should not come as a huge surprise. There is no noise in the data and it is a relatively simple function to emulate. But it's still kind of cool -- we didn't have to spend a lot of time hand-crafting weights _etc._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Side Note: Call-back Functions\n",
    "\n",
    "If you want to interrogate some internal details of your model, you may require the use of a _call-back function_. This is a function triggered by some specified criteria.\n",
    "\n",
    "In the following case, we wanted to look at how the weights of the model evolved with each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 6         \n",
      "=================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test loss: 1.1920928955078125e-07,      Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "\n",
    "input_size = 2 \n",
    "model100 = Sequential()\n",
    "model100.add(Dense(input_size, activation='relu', input_shape=(2,), use_bias=True))\n",
    "model100.add(Dense(num_classes, activation='sigmoid'))\n",
    "model100.summary()\n",
    "\n",
    "# Create a callback function to populate a dictionary of weights\n",
    "weights = {}\n",
    "store_weights = LambdaCallback(on_epoch_end=lambda batch,\n",
    "                               logs: weights.__setitem__(batch,\n",
    "                                                       model100.layers[0].get_weights()) )\n",
    "\n",
    "model100.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history100 = model100.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                         callbacks = [store_weights])\n",
    "\n",
    "score100 = model100.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {0},      Test accuracy: {1}'.format(score100[0], score100[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_names = []\n",
    "\n",
    "for Idx in range(3):\n",
    "    for Jdx in range(input_size):\n",
    "        weights_names.append(\"node\" + str(Jdx) + \"_\" + str(Idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Idx in weights:\n",
    "    weights[Idx] = np.concatenate(\n",
    "        (weights[Idx][0][0], weights[Idx][0][1], weights[Idx][1]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_df = pd.DataFrame(weights).T\n",
    "weights_df.columns = weights_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 6)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node0_0</th>\n",
       "      <th>node1_0</th>\n",
       "      <th>node0_1</th>\n",
       "      <th>node1_1</th>\n",
       "      <th>node0_2</th>\n",
       "      <th>node1_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.378349</td>\n",
       "      <td>1.026016</td>\n",
       "      <td>0.378585</td>\n",
       "      <td>-0.076626</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>-0.042507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.468927</td>\n",
       "      <td>0.967750</td>\n",
       "      <td>0.468637</td>\n",
       "      <td>-0.216440</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.100772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.558707</td>\n",
       "      <td>0.918462</td>\n",
       "      <td>0.557364</td>\n",
       "      <td>-0.359163</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.150060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.642713</td>\n",
       "      <td>0.868215</td>\n",
       "      <td>0.642518</td>\n",
       "      <td>-0.501522</td>\n",
       "      <td>-0.000222</td>\n",
       "      <td>-0.200307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.723031</td>\n",
       "      <td>0.854663</td>\n",
       "      <td>0.723476</td>\n",
       "      <td>-0.639140</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>-0.213858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    node0_0   node1_0   node0_1   node1_1   node0_2   node1_2\n",
       "0 -0.378349  1.026016  0.378585 -0.076626  0.000972 -0.042507\n",
       "1 -0.468927  0.967750  0.468637 -0.216440 -0.000133 -0.100772\n",
       "2 -0.558707  0.918462  0.557364 -0.359163 -0.000119 -0.150060\n",
       "3 -0.642713  0.868215  0.642518 -0.501522 -0.000222 -0.200307\n",
       "4 -0.723031  0.854663  0.723476 -0.639140  0.000695 -0.213858"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node0_0</th>\n",
       "      <th>node1_0</th>\n",
       "      <th>node0_1</th>\n",
       "      <th>node1_1</th>\n",
       "      <th>node0_2</th>\n",
       "      <th>node1_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>-2.389724</td>\n",
       "      <td>2.700161</td>\n",
       "      <td>2.38846</td>\n",
       "      <td>-2.70169</td>\n",
       "      <td>-0.000339</td>\n",
       "      <td>-0.000559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>-2.389724</td>\n",
       "      <td>2.700161</td>\n",
       "      <td>2.38846</td>\n",
       "      <td>-2.70169</td>\n",
       "      <td>-0.000339</td>\n",
       "      <td>-0.000559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>-2.389724</td>\n",
       "      <td>2.700161</td>\n",
       "      <td>2.38846</td>\n",
       "      <td>-2.70169</td>\n",
       "      <td>-0.000339</td>\n",
       "      <td>-0.000559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>-2.389724</td>\n",
       "      <td>2.700161</td>\n",
       "      <td>2.38846</td>\n",
       "      <td>-2.70169</td>\n",
       "      <td>-0.000339</td>\n",
       "      <td>-0.000559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>-2.389724</td>\n",
       "      <td>2.700161</td>\n",
       "      <td>2.38846</td>\n",
       "      <td>-2.70169</td>\n",
       "      <td>-0.000339</td>\n",
       "      <td>-0.000559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      node0_0   node1_0  node0_1  node1_1   node0_2   node1_2\n",
       "507 -2.389724  2.700161  2.38846 -2.70169 -0.000339 -0.000559\n",
       "508 -2.389724  2.700161  2.38846 -2.70169 -0.000339 -0.000559\n",
       "509 -2.389724  2.700161  2.38846 -2.70169 -0.000339 -0.000559\n",
       "510 -2.389724  2.700161  2.38846 -2.70169 -0.000339 -0.000559\n",
       "511 -2.389724  2.700161  2.38846 -2.70169 -0.000339 -0.000559"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_df.shape\n",
    "weights_df.head()\n",
    "weights_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6fe3cb84a8>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights_df.filter(regex=\"node[0-3]_0\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6fe25d7a58>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights_df.filter(regex=\"node[0-3]_1\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6fe25af320>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights_df.filter(regex=\"node[0-3]_2\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may add a hidden layer by simply repeating most of the initial part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "# 1st (Input) layer\n",
    "model1.add(Dense(input_size, activation='relu', input_shape=(2,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the 2nd (hidden) layer. This is an internalization of features that are not seen externally to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.add(Dense(64, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And retain the remainder (the output layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                192       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 328\n",
      "Trainable params: 328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test loss: 1.1920928955078125e-07,      Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 3rd (Output) layer\n",
    "model1.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history1 = model1.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=128,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score1 = model1.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss: {0},      Test accuracy: {1}'.format(score1[0], score1[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that adding a layer decreased the validation (test) loss dramatically, and improved the accuracy.\n",
    "\n",
    "Adding layers increases the network _depth_. As the number of hidden layers increase, the network becomes deeper; this is what is referred to as _Deep Learning_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network dropout\n",
    "\n",
    "Overfitting is an ever-present issue with machine learning models. One means of reducing overfitting is to induce _network dropout_. This involves selecting a subset of the model inputs at random during each training phase. This is simply done in keras, setting the `rate` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.layers.Dropout(0.2)  # Induces a 20% drop-out rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add drop-out to the first two layers of our MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                192       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 328\n",
      "Trainable params: 328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test loss: 0.04022257467162269,      Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "# 1st (Input) layer\n",
    "model2.add(Dense(input_size, activation='relu', input_shape=(2,)))\n",
    "model2.add(Dropout(0.1))\n",
    "\n",
    "# 2nd (Hidden) layer\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dropout(0.1))\n",
    "\n",
    "# 3rd (Output) layer\n",
    "model2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history2 = model2.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=128,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score2 = model2.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss: {0},      Test accuracy: {1}'.format(score2[0], score2[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the width of the model\n",
    "\n",
    "Note the width on the above models is 16 neurons. This is not many! Let's increase this to 512 for the non-output layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 512)               1536      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 265,218\n",
      "Trainable params: 265,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model 3 summary: None\n",
      "Test loss: 1.1920928955078125e-07,      Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "model3 = Sequential()\n",
    "# 1st (Input) layer\n",
    "model3.add(Dense(512, activation='relu', input_shape=(2,)))\n",
    "model3.add(Dropout(0.2))  # Increased the drop-out rate\n",
    "\n",
    "# 2nd (Hidden) layer\n",
    "model3.add(Dense(512, activation='relu'))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "# 3rd (Output) layer\n",
    "model3.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "print(\"Model 3 summary: {0}\".format(model3.summary()))\n",
    "\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history3 = model3.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score3 = model3.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss: {0},      Test accuracy: {1}'.format(score3[0], score3[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So increasing the model's depth improved the validation accuracy and reduced the error.\n",
    "\n",
    "It is interesting to see how the training and validation errors of each of these models improves with each epoch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Comparison of the four models as a function of epoch. Note the decrease in error rate with epoch is rapid in the early stages of training.](images/Perceptron_training_comparisons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appropriate applications of Artificial Neural Networks\n",
    "\n",
    "Because ANNs are 'universal approximators,' as well as based on a large number of small, simple, units, they are great where:\n",
    " * The relationships between variables are poorly understood or analytically complex\n",
    " * There is a lot of data\n",
    " \n",
    "They are not so great because:\n",
    " * Principal features are not explicitly apparent; decisions are opaque for deep networks\n",
    " * They can be slow to train, requiring a number of epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional resources\n",
    "\n",
    "\n",
    "**Websites:** \n",
    "\n",
    "  * Michael A. Neilsen's _Neural Nets and Deep Learning_: http://neuralnetworksanddeeplearning.com/\n",
    "  * Ian Goodfellow, Yoshua Bengio and Aaron Courville's _Deep Learning_: http://www.deeplearningbook.org\n",
    "\n",
    "**YouTube channels:**\n",
    "\n",
    " * Andrew Ng's _Machine Learning_: https://www.youtube.com/watch?v=PPLop4L2eGk&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN\n",
    " * Sentdex's _Practical Machine Learning with Python_: https://www.youtube.com/watch?v=OGxgnH8y2NM&index=1&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v\n",
    "\n",
    "**Platforms:** \n",
    "\n",
    " * Kaggle: https://www.kaggle.com/\n",
    " * Coursera (Andrew Ng again): https://www.coursera.org/learn/machine-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "\n",
    "This was a very brief introduction to the field of artificial neural networks (ANNs) and Deep Learning!\n",
    "\n",
    "We have examined the theoretical justification for (ANNs), demonstrating that they are great ‘universal approximators’. We also covered their use-cases and some of their pit-falls.\n",
    "\n",
    "We also had a brief introduction to TensorFlow and Keras. We built a feed-forward network (a Multi-Layer Perceptron; MLP) to approximate complex functions. We ‘tweaked’ this model, improving the output, and evaluated its performance. In order to determine this, we also covered the concepts of appropriate activation functions, optimizers and cost functions.\n",
    "\n",
    "Perhaps most importantly: we have figured out how to be satisfied with a simple meal at an altitude of 30,000 feet. People have been trying to achieve this for years!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
