{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The chicken or the beef? A justification for artificial neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The purpose of this section is to cover the foundations of Artificial Neural Networks (ANNs), as well as their relation to Deep Learning. We will first give an overview of various neural net architectures and then implement some key models using Keras and Tensorflow. We will also look at how to assess model performance and how to persist these models for future use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, we cover here:\n",
    "\n",
    "* A conceptual introduction to Artificial Neural Networks (ANNs)\n",
    "* TensorFlow and Keras overview\n",
    "* Defining the base input\n",
    "* Building models by adding defined layers\n",
    "* Loss functions, optimizers, and metrics\n",
    "* Training models \n",
    "* Performance assessment\n",
    "* Improving performance by increasing network depth and width, and decreasing overfitting by introducing network drop-out\n",
    "* Persisting Keras models for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries for manipulation and plotting \n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ra/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable inline plotting for graphics\n",
    "%matplotlib inline\n",
    "## Set larger default figure size\n",
    "matplotlib.rcParams['figure.figsize'] = [10.0,6.0]\n",
    "## Multiple outputs from cells\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \n",
      "[GCC 7.2.0]\n",
      "Pandas version: 0.23.0\n",
      "Matplotlib version: 2.2.2\n",
      "Numpy version: 1.14.3\n",
      "Tensorflow version: 1.10.0\n"
     ]
    }
   ],
   "source": [
    "## Get Version information\n",
    "print(sys.version)\n",
    "print(\"Pandas version: {0}\".format(pd.__version__))\n",
    "print(\"Matplotlib version: {0}\".format(matplotlib.__version__))\n",
    "print(\"Numpy version: {0}\".format(np.__version__))\n",
    "print(\"Tensorflow version: {0}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most basic artificial neuron: the Perceptron\n",
    "\n",
    "Let's say you have an important decision to make. You are hungry and wish to be sated. However, you are on an airplane, so your options are 'chicken' or 'beef'. Your target variable is 'satisfaction,' which will not be achieved if you choose neither, or both, options. You must make this decision while you have the flight attendant's attention. Finally, the attendant is new and very busy, and may not recall your first choice, so you will have to be explicit about what you _do not_ wish, as well as what you do (or perhaps they ran out of one option and another attendant has to provide this option later, without information on the first meal option).\n",
    "\n",
    "This contrived scenario is simple by design. Yet how would we go about modeling this decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could approach this as a linear regression problem: $f(x) = w_1x_1 + w_2x_2 + b$, where the $w_i$ are the weights of the inputs $x_i$, and minimize the least squares cost function, $J(x) = \\sum_{r} (f'(x) - f(x))^2$, where the $r$ run over all the input values.\n",
    "\n",
    "However, consider an intrinsic problem here. The final decision depends wholly on the interaction between the two input variables, 'chicken' (say, $x_1$) or 'beef' ($x_2$):\n",
    "\n",
    "![Decision plane for meals... on a plane](images/chicken_or_beef.png)\n",
    "\n",
    "If $x_1$ is maximal, we wish the value of $x_2$ to be minimal, and vice-versa. But we can't just change the coefficients from a regression at will! In other words, there is no single line that separates the Chicken-Beef ($x_1-x_2$) plane to define a distinct _decision boundary_ between the two classes, 'sated' and 'not sated'. However, we may do this with _two_ lines (and logical comparisons):\n",
    "\n",
    "![Division of decision plane using two linear regions](images/chicken_or_beef_twoLines.png)\n",
    "\n",
    "The lower (green) line marks a boundary between the (0, 0) point and the other three, containing all but the origin point. The upper (blue) line defines a region that contains only the (1, 1) point.\n",
    "\n",
    "In other words, the regions defined by:\n",
    "\n",
    "$x_2 \\geq -x_1 + 0.5$\n",
    "\n",
    "$x_2 \\geq -x_1 + 1.2$\n",
    "\n",
    "each return 1 if the condition is met. This may be re-cast:\n",
    "\n",
    "$x_1 + x_2 \\geq 0.5 \\rightarrow H(x_1 + x_2 - 0.5)$\n",
    "\n",
    "$x_1 + x_2 \\geq 1.2 \\rightarrow H(x_1 + x_2 - 1.2)$\n",
    "\n",
    "The simplest function to return a 1 or 0 depending on a fixed criterion, is the Heaviside step-function, $H(x)$, with the following properties:\n",
    "$H(x) = \\{ ^{1\\textrm{ for }x > 0} _{0\\textrm{ for x } \\leq 0}$\n",
    "\n",
    "A schematic allowing these two lines to interact with each other, for this comparison may look like:\n",
    "\n",
    "![Schematic of algorithm determining membership of classification regions ('sated' and 'not sated')](images/Simple_MLP_figure.png)\n",
    "\n",
    "Where the last calculation is a simple logical comparison (`and`) of the regions.\n",
    "\n",
    "This, which is really the construction of the XOR function, is the essential idea behind the most fundamental neural net: the perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks (ANNs)\n",
    "\n",
    "Artificial Neural Networks (ANNs; often further abbreviated to NNs) are inspired directly from our understanding of brain neurophysiology. An individual _neuron_ is the basic cell unit of our complex central nervous system. Each neuron takes inputs, in the form of electrical signals, and performs a number of simple transforms on these inputs, resulting in a simple output. These outputs are in turn fed as inputs to other neurons.\n",
    "\n",
    "![Image of a neuron (public domain: https://commons.wikimedia.org/wiki/File:Neuron.jpg)](images/Neuron.jpg)\n",
    "\n",
    "Artificial neurons are simplified analogs of these biological units, taking in a limited number of signals, performing simple operations on them, before emitting a limited number of output signals. The astounding computational capabilities of this class of algorithm arise from the networks built up using these simple units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of a Neural Network \n",
    "\n",
    "An ANN is composed of _neurons_ (_nodes_) and _layers_. Each node performs the atomic operations of the network, defined by _activation functions_. Groups of nodes may form a layer, a distinct structure representing a stage of the network. Each layer acts like a filter, or function. At least two layers are defined: the _input layer_ and the _output layer_. In addition, there may be one or more layer that is neither an input nor an output; these are referred to as _hidden layers_:\n",
    "\n",
    "\n",
    "\n",
    "![Achitecture of a typical Artificial Neural Network (ANN). The number of layers determine the _depth_ of the model; the number of neurons in each layer determine the _width_ of the model.](images/ANN_architecture_intro.png)\n",
    "\n",
    "The purpose of ANNs is to approximate any arbitrary function, say, $f'(x)$. Each layer can be thought of as a successive function $f_i()$ acting on the previous layers. The particular composition of layers and nodes of a neural network is known as the _net architecture_.\n",
    "\n",
    "In this framework, for the chicken-beef calculation we performed above, each linear comparison was performed within a node (neuron), after being fed inputs $x_1$ and $x_2$. The outputs were fed into the final, output, node. This architecture is a Multi-Layer Perceptron (MLP). It is a particular type of _feed-forward network_, because there are no layers that make use of feedback.\n",
    "\n",
    "The activation function we chose (fairly organically!) was the Heaviside step function, $H(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "The purpose of an activation function is to polarize the network (_i.e._ provide directionality), as well as condition the signals propagated throughout (very often regulated to have a limited output range). The most common activation functions are:\n",
    "\n",
    " *  **Perceptron** (Heaviside): $\\sigma(z) = \\{ ^{1\\textrm{ for }z > 0} _{0\\textrm{ for z } \\leq 0}$\n",
    "\n",
    " *  **Sigmoid** (logistic): $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$\n",
    " \n",
    " * **ReLU** (Rectified Linear Unit): $\\sigma(z) = \\max{(0, z)}$\n",
    "\n",
    " *  **Softmax**: $\\sigma(z)_j = \\frac{\\exp{(z_j)}}{\\sum_{k}^{K} \\exp{(z_k)}}$\n",
    " \n",
    "Their response functions look like the following:\n",
    "\n",
    "![Response functions of three of the most popular activation functions for neurons in ANNs.](images/activation_function_comparison.png)\n",
    "\n",
    "The sigmoid (or logistic) activation function is a smoothed version of the step function, so has nicer analytic properties than the step function. However, it can be somewhat computationally expensive for large numbers of nodes and layers. The ReLU (Rectified Linear Unit) is a simpler function. Although, being piece-wise linear, it is still technically non-linear, it provides network polarity while retaining many properties of linearity which make these nice for approximating functions. ReLU-based neurons are much 'faster' to train because of their computational simplicity.\n",
    "\n",
    "The softmax activation function is an ensemble function, often used for an aggregation step. It also has nice analytic properties, regulating the output based on the ensemble mean. This often favors a 'winner-takes-all' condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So much for the theory! How do we code these things?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Keras and Tensorflow\n",
    "\n",
    "\n",
    "**Tensorflow**\n",
    "\n",
    "TensorFlow was developed by the Google Brain team, released to the Apache foundation in late 2015. It is a symbolic, high-performance, math library with specialized and generalized math objects, particularly _tensors_, a generalization of vector arithmetic and calculus (hence the name). The mental model for TensorFlow computations is a computational graph, defined by tensors. It is designed to be seamlessly applied to a range of hardware types (including GPGPUs and a specialized ASIC, the TPU---_Tensor Processing Unit_).  \n",
    "\n",
    "**Tensorflow documentation:** https://www.tensorflow.org/\n",
    "\n",
    "---\n",
    "**Keras**\n",
    "\n",
    "Keras is a high-level API to the neural network libraries CNTK, Theano and TensorFlow. Its high level of abstraction allows rapid prototyping of neural networks, with both convolution and recurrent network architectures. Its guiding principles are user-friendliness, modularity and to be easily extendible. Because it's written in Python, configuration and extension of functionality are relatively seamless within the Python eco-system. \n",
    "\n",
    "Keras is Greek for 'horn,' a reference to the vision-inducing spirits in the _Odyssey_.\n",
    "\n",
    "**Keras documentation:** https://keras.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and TensorFlow\n",
    "\n",
    "The fundamental unit of computation within TensorFlow is the _tensor_. Tensors are generalizations of vector arithmetic and calculus, allowing linear operations on higher rank objects. These are used to partially define a computation, in the form of a data-flow graph, that will, when executed, produce an output value. TensorFlow constructs a graph based on tensor objects (`tf.Tensor`). This graph is then executed within a TensorFlow session (`tf.Session()`) instance.\n",
    "\n",
    "We can simply generate a tensor object using `tf.Variable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "odd_nums = tf.Variable([1, 3, 5, 7, 9, 11])  # Rank 1 tensor is a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has the usual `.dtype` and `.shape` attributes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.int32_ref"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(6)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_nums.dtype\n",
    "odd_nums.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weird_hypercube = tf.Variable([ [ [43], [121.234] ], [ [987], [2134] ] ], dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as mentioned, tensors in TensorFlow are _partially_ computed graph objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Rank:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank1 = tf.rank(odd_nums)\n",
    "rank1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Rank_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank2 = tf.rank(weird_hypercube)\n",
    "rank2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_1:0' shape=(2, 2, 1) dtype=float64_ref>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weird_hypercube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations are not run until we have specified that our computation graph is complete. Note that the `tf.rank()` operations did not return an actual rank value. To execute operations on the tensors we just produced, we require a `tf.Session()` connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(rank1)\n",
    "    sess.run(rank2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the details of your computation this way, and generating the necessary graphs and sessions, requires quite some thought and a lot of boiler-plate code (of which is not particularly Pythonic!). \n",
    "\n",
    "There are so many commonly used operations and architectures, that a higher level API would be very useful! This is where Keras comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Multi-Level Perceptron (MLP) model in Keras\n",
    "\n",
    "We will now generate a simple Multi-Level Perceptron (MLP) model to build a classifier of an arbitrary---but complicated---function: \n",
    "\n",
    "![Figure of the nasty function we wish to approximate with a feed-forward network algorithm (Multi-Layer Perceptron; MLP)](images/Simple_Perceptron_comparison.png)\n",
    "\n",
    "This somewhat nasty function (OK, it's not that bad in the scheme of things) was generated from the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 100, 0.01)\n",
    "\n",
    "L = len(x)\n",
    "y1 = np.zeros(L)\n",
    "y2 = np.zeros(L)\n",
    "y3 = np.zeros(L)\n",
    "\n",
    "# Produce piece-wise indices\n",
    "x1_Idx = np.where(x < 40)\n",
    "x2_Idx = np.where((x > 30) & (x < 80))\n",
    "x3_Idx = np.where(x > 50)\n",
    "\n",
    "# Generate arbitrary piece-wise functions\n",
    "\n",
    "y1[x1_Idx] = [5 for Idx in x1_Idx]\n",
    "y2[x2_Idx] = [pow(Idx - 55, 2) for Idx in x[x2_Idx]]\n",
    "y3[x3_Idx] = [pow(Idx - 75, 4) for Idx in x[x3_Idx]]\n",
    "\n",
    "y2 = 3*y2/np.max(y2)\n",
    "y3 = 3*y3/np.max(y3)\n",
    "\n",
    "# Add some noise\n",
    "n = np.random.randn(L)\n",
    "n = n/np.max(n)\n",
    "\n",
    "y_tot = y1+y2+y3+n\n",
    "y_thresh = np.median(y_tot)\n",
    "y_out = np.where(y_tot > y_thresh, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to classify this function based on three factors, $y_1$, $y_2$ and $y_3$, comparing their sum to a threshold. \n",
    "\n",
    "\n",
    "We will do this using an MLP feed-forward network in Keras. First, we will have to perform some additional imports of modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some additional imports\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "num_classes = 2  # Binary classifier\n",
    "epochs = 7  # Number of rounds of training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_Idx = np.random.choice(range(x.shape[0]), int(0.75*x.shape[0]))\n",
    "\n",
    "x = np.vstack([y1, y2, y3, n]).T  # Make each variable a feature vector \n",
    "\n",
    "x_train = x[split_Idx]\n",
    "x_test = x[~split_Idx]  # Note the bit-wise logical negation\n",
    "y_train = y_out[split_Idx]\n",
    "y_test = y_out[~split_Idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(7500,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(7500, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(7500,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n",
    "y_train.shape\n",
    "x_test.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has two types of model: `Sequential` and the functional `Model`. \n",
    "Each model type shares the following attributes:\n",
    "\n",
    "* `.layers`: the layers of the model\n",
    "* `.inputs`: the input tensors of the model\n",
    "* `.outputs`: the output tensors\n",
    "\n",
    "They also have a `.summary()` method, giving a summary of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to call our first model! Instantiate an instance of the `Sequential()` model class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the initial (input) layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.add(Dense(16, activation='relu', input_shape=(4,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the final (output) layer:                                                                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have thus defined our model's architecture. We must now define _how_ the model determines it has found a suitable approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost and loss functions\n",
    "\n",
    "The important objective functions for ANNs are referred to as _cost_ or _loss_ functions. These functional measures of error are the important metrics with which we determine the success of our algorithm. The goal of machine learning algorithms is to optimize such a functions. The former term (_cost function_) is often reserved for the entire training set, in which case the _loss function_ is defined as the loss per epoch. \n",
    "\n",
    "We are already familiar with one of the most common cost functions, namely the Mean Squared Error (MSE). This is the routine used to obtain an Ordinary Least Squares (OLS) linear regression fit. Explicitly, we minimize the function:\n",
    "\n",
    "$J(x) = \\frac{1}{2n}\\sum{}^{}_{j} [f'_j(x) - f_j(x)]^2$\n",
    "\n",
    "Where $f'(x)$ denotes the function to be approximated and $f(x)$ represents the functional form used by the algorithm (here, the activation function used in each layer). Here, the sum is taken over each node, labeled with $j$. \n",
    "\n",
    "Another cost function, popularly used for binary classification, is the _cross-entropy_ (or Bernoulli negative log-likelihood or Binary Cross-Entropy):\n",
    "\n",
    "$J(x) = -\\sum{}_{j}[f(x)\\log_e{(f'(x))} + (1 - f(x))\\log_e{(1 - f'(x))}]$\n",
    "\n",
    "One problem with using the MSE cost function is that it can be very slow to learn with large errors for a sigmoid activation function. This is not the case for cross-entropy, hence it is widely used.\n",
    "\n",
    "There are a number of other cost functions, such as the Mean Absolute Error (MAE):\n",
    "\n",
    "$J(x) = \\frac{1}{n}\\sum{}^{}_{j} |f'_j(x) - f_j(x)|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "In practice, we do not usually have access to the analytic form of the cost or loss functions, and hence do not have an explicit expression for the optimal parameter values. We have to then rely on optimization schemes. Probably the best known are the Newton-Raphson family of optimization functions, which 'descend' to the optimal point, based on the gradient (this is known as _gradient descent_). \n",
    "\n",
    "Specifically, perhaps the most widely used optimizers for ANNs are based on _Stochastic Gradient Descent_ (SGD). Consider that the goal here is to minimize the cost function, $J(x)$. In other words, where $\\nabla J(x) = 0$ \n",
    "\n",
    "(Note that here we have adopted the gradient operator,$\\nabla$ ('nabla'); although we have written the above as a function of a single variable $\\nabla f(x) := \\partial f(x)/\\partial x$, $\\nabla$ is the derivative across all variables in the space.)  \n",
    "\n",
    "As an iterative process, we update the weights $w^{(r)}$, beginning from the initial $w^{(0)}$. We wish to find $\\nabla J(x)$; this may be well approximated by taking a random sample of training inputs and computing the (discrete) gradient by taking a group of random nodes (a _mini-batch_), $j \\in m$. In other words, we assume $\\nabla J(x) \\approx \\nabla J_m(x)$. \n",
    "\n",
    "The $r$th iteration of a weight is hence updated according to: $w^{(r)} = w^{(r-1)} - \\eta\\nabla J_m(w)$. The constant parameter, $\\eta > 0 $, is known as the _Learning Rate_, as it determines the rate at which the weights are updated.\n",
    "\n",
    "To help with numerical stability, the concept of _momentum_ has been introduced. This reduces oscillations, and overshoot of the global minimum, by introducing a term proportional to the incremental change in rate. Another useful parameter within enhancements to SGD optimization is the _decay_ parameter. This reduces the learning rate if the loss does not decrease after a set number of epochs. \n",
    "\n",
    "Even more sophisticated variants of SGD involve the automatic update of learning rates depending on how important a particular feature parameter is (Adagrad, Adadelta). We will use here the optimizer recommended as a great all-purpose default, the `RMSProp()` variant of Adadelta. In effect, this means that we do not have to attempt to tune the learning rate; this is done so automatically. \n",
    "\n",
    "For more details on variants of optimizers used in ANNs, I recommend Sebastian Ruder's _An overview of gradient descent optimization algorithms_:\n",
    " \n",
    "http://ruder.io/optimizing-gradient-descent/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizers.SGD at 0x7f89dcca9c18>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking model performance: metrics\n",
    "\n",
    "It is vital to quantify how our models perform. Keras makes it simple to track a number of off-the-shelf loss functions, that are not used to update or train the model, but may elucidate its behavior. This may range from the simple `accuracy` (the mean difference between prediction and actual 'ground truth' values), mean absolute error (`mae`) or `categorical_accuracy`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within Keras, it is a simple matter to define the loss and optimizer functions, and performance metric to track for our MLP model. These are specified at the `compile` stage of the computation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 7500 samples\n",
      "Epoch 1/7\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.4294 - acc: 0.8124 - val_loss: 0.3635 - val_acc: 0.9133\n",
      "Epoch 2/7\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3309 - acc: 0.9051 - val_loss: 0.2957 - val_acc: 0.9001\n",
      "Epoch 3/7\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.2702 - acc: 0.9048 - val_loss: 0.2401 - val_acc: 0.9077\n",
      "Epoch 4/7\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.2165 - acc: 0.9204 - val_loss: 0.1892 - val_acc: 0.9383\n",
      "Epoch 5/7\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.1700 - acc: 0.9467 - val_loss: 0.1470 - val_acc: 0.9555\n",
      "Epoch 6/7\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.1318 - acc: 0.9673 - val_loss: 0.1135 - val_acc: 0.9793\n",
      "Epoch 7/7\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.1042 - acc: 0.9827 - val_loss: 0.0914 - val_acc: 0.9847\n"
     ]
    }
   ],
   "source": [
    "history0 = model0.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.09144360537131628,      Test accuracy: 0.9846666666666667\n"
     ]
    }
   ],
   "source": [
    "score0 = model0.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {0},      Test accuracy: {1}'.format(score0[0], score0[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained a model on an arbitrary function, with an apparent accuracy of 98.1%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may add a hidden layer by simply repeating most of the initial part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "# 1st (Input) layer\n",
    "model1.add(Dense(16, activation='relu', input_shape=(4,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the 2nd (hidden) layer. This is an internalization of features that are not seen externally to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.add(Dense(16, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And retain the remainder (the output layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 386\n",
      "Trainable params: 386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7500 samples, validate on 7500 samples\n",
      "Epoch 1/7\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.4846 - acc: 0.7465 - val_loss: 0.3742 - val_acc: 0.8861\n",
      "Epoch 2/7\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.3175 - acc: 0.9235 - val_loss: 0.2656 - val_acc: 0.9111\n",
      "Epoch 3/7\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.2274 - acc: 0.9244 - val_loss: 0.1892 - val_acc: 0.9479\n",
      "Epoch 4/7\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.1570 - acc: 0.9612 - val_loss: 0.1258 - val_acc: 0.9743\n",
      "Epoch 5/7\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.1054 - acc: 0.9807 - val_loss: 0.0846 - val_acc: 0.9976\n",
      "Epoch 6/7\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.0734 - acc: 0.9911 - val_loss: 0.0601 - val_acc: 0.9953\n",
      "Epoch 7/7\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.0551 - acc: 0.9929 - val_loss: 0.0464 - val_acc: 0.9972\n",
      "Test loss: 0.04642097859084606,      Test accuracy: 0.9972\n"
     ]
    }
   ],
   "source": [
    "# 3rd (Output) layer\n",
    "model1.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history1 = model1.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score1 = model1.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss: {0},      Test accuracy: {1}'.format(score1[0], score1[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that adding a layer decreased the validation (test) loss dramatically, and improved the accuracy.\n",
    "\n",
    "Adding layers increases the network _depth_. As the number of hidden layers increase, the network becomes deeper; this is what is referred to as _Deep Learning_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network dropout\n",
    "\n",
    "Overfitting is an ever-present issue with machine learning models. One means of reducing overfitting is to induce _network dropout_. This involves selecting a subset of the model inputs at random during each training phase. This is simply done in keras, setting the `rate` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dropout at 0x7f89f158de80>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras.layers.Dropout(0.2)  # Induces a 20% drop-out rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add drop-out to the first two layers of our MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 386\n",
      "Trainable params: 386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7500 samples, validate on 7500 samples\n",
      "Epoch 1/7\n",
      "7500/7500 [==============================] - 1s 131us/step - loss: 0.4367 - acc: 0.8077 - val_loss: 0.3428 - val_acc: 0.9029\n",
      "Epoch 2/7\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.3088 - acc: 0.9161 - val_loss: 0.2523 - val_acc: 0.9011\n",
      "Epoch 3/7\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.2225 - acc: 0.9349 - val_loss: 0.1703 - val_acc: 0.9468\n",
      "Epoch 4/7\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.1569 - acc: 0.9559 - val_loss: 0.1156 - val_acc: 0.9693\n",
      "Epoch 5/7\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.1165 - acc: 0.9619 - val_loss: 0.0806 - val_acc: 0.9852\n",
      "Epoch 6/7\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.0941 - acc: 0.9693 - val_loss: 0.0607 - val_acc: 0.9964\n",
      "Epoch 7/7\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.0761 - acc: 0.9728 - val_loss: 0.0496 - val_acc: 0.9968\n",
      "Test loss: 0.049639763354261714,      Test accuracy: 0.9968\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "# 1st (Input) layer\n",
    "model2.add(Dense(16, activation='relu', input_shape=(4,)))\n",
    "model2.add(Dropout(0.1))\n",
    "\n",
    "# 2nd (Hidden) layer\n",
    "model2.add(Dense(16, activation='relu'))\n",
    "model2.add(Dropout(0.1))\n",
    "\n",
    "# 3rd (Output) layer\n",
    "model2.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history2 = model2.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score2 = model2.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss: {0},      Test accuracy: {1}'.format(score2[0], score2[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the width of the model\n",
    "\n",
    "Note the width on the above models is 16 neurons. This is not many! Let's increase this to 512 for the non-output layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 512)               2560      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 266,242\n",
      "Trainable params: 266,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model 3 summary: None\n",
      "Train on 7500 samples, validate on 7500 samples\n",
      "Epoch 1/7\n",
      "7500/7500 [==============================] - 3s 359us/step - loss: 0.1441 - acc: 0.9433 - val_loss: 0.0370 - val_acc: 0.9949\n",
      "Epoch 2/7\n",
      "7500/7500 [==============================] - 2s 325us/step - loss: 0.0422 - acc: 0.9824 - val_loss: 0.0272 - val_acc: 0.9900\n",
      "Epoch 3/7\n",
      "7500/7500 [==============================] - 3s 341us/step - loss: 0.0352 - acc: 0.9852 - val_loss: 0.0171 - val_acc: 0.9969\n",
      "Epoch 4/7\n",
      "7500/7500 [==============================] - 2s 292us/step - loss: 0.0277 - acc: 0.9883 - val_loss: 0.0145 - val_acc: 0.9960\n",
      "Epoch 5/7\n",
      "7500/7500 [==============================] - 2s 269us/step - loss: 0.0262 - acc: 0.9893 - val_loss: 0.0129 - val_acc: 0.9967\n",
      "Epoch 6/7\n",
      "7500/7500 [==============================] - 2s 289us/step - loss: 0.0243 - acc: 0.9889 - val_loss: 0.0117 - val_acc: 0.9979\n",
      "Epoch 7/7\n",
      "7500/7500 [==============================] - 2s 283us/step - loss: 0.0206 - acc: 0.9917 - val_loss: 0.0101 - val_acc: 0.9971\n",
      "Test loss: 0.010115630360713597,      Test accuracy: 0.9970666666666667\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "model3 = Sequential()\n",
    "# 1st (Input) layer\n",
    "model3.add(Dense(512, activation='relu', input_shape=(4,)))\n",
    "model3.add(Dropout(0.2))  # Increased the drop-out rate\n",
    "\n",
    "# 2nd (Hidden) layer\n",
    "model3.add(Dense(512, activation='relu'))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "# 3rd (Output) layer\n",
    "model3.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "print(\"Model 3 summary: {0}\".format(model3.summary()))\n",
    "\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history3 = model3.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score3 = model3.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss: {0},      Test accuracy: {1}'.format(score3[0], score3[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So increasing the model's depth improved the validation accuracy and reduced the error.\n",
    "\n",
    "It is interesting to see how the training and validation errors of each of these models improves with each epoch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Comparison of the four models as a function of epoch. Note the decrease in error rate with epoch is rapid in the early stages of training.](images/Perceptron_training_comparisons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An overview of network architectures\n",
    "\n",
    "A great pictorial summary of the various architectures may be found in Fjodor van Veen's article, \"The Neural Network Zoo\" (http://www.asimovinstitute.org/neural-network-zoo/). I reproduce it here without permission; however I highly recommend reading the whole article:\n",
    "\n",
    "![The Neural Network Zoo (credit: Fjodor van Veen)](images/Asimov_neuralnetworks_architectures.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persisting Keras models for future use\n",
    "\n",
    "Once you have a model you are satisfied with, you may save and distribute it by serialization. The Keras documentation **does not recommend pickling models**. However there are a number of other methods for common file formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the entire model (_i.e._ architecture and weights) to Hierarchical Data Format (HDF5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1bba752af35f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model0_saved.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmodel0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model0' is not defined"
     ]
    }
   ],
   "source": [
    "model0.save('model0_saved.h5')\n",
    "del model0\n",
    "model0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('model0_saved.h5')]\n"
     ]
    }
   ],
   "source": [
    "# Check the files were created\n",
    "print(list(pathlib.Path().glob('*.h5')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-constituting the model is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "loaded_model0 = load_model('model0_saved.h5')\n",
    "loaded_model0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialization of the model architecture to JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1700"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_json = model1.to_json()\n",
    "with open(\"model1.json\", \"w\") as model_file:\n",
    "    model_file.write(model1_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('model0.json'), PosixPath('model1.json')]\n"
     ]
    }
   ],
   "source": [
    "# Check the files were created\n",
    "print(list(pathlib.Path().glob('*.json')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialization of the weights to Hierarchical Data Format (HDF5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save_weights(\"model1_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('model0_saved.h5'), PosixPath('model1_weights.h5')]\n"
     ]
    }
   ],
   "source": [
    "# Check the files were created\n",
    "print(list(pathlib.Path().glob('*.h5')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model from the resulting JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 386\n",
      "Trainable params: 386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with open('model1.json', 'r') as model_file:\n",
    "    model1_loaded = model_file.read()\n",
    "    \n",
    "from tensorflow.keras.models import model_from_json\n",
    "loaded_model = model_from_json(model1_loaded)\n",
    "loaded_model.summary()\n",
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])  # Compile the graph, without training the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the weights from HDF5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_weights(\"model1_weights.h5\")  \n",
    "# This saves the weights of the trained model too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the persisted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 386\n",
      "Trainable params: 386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Test loss: 1.4468912439982096,      Test accuracy: 0.4953333333174388\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()\n",
    "score = loaded_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {0},      Test accuracy: {1}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between TensorFlow+Keras and PyTorch\n",
    "\n",
    "PyTorch is a similar platform to TensorFlow. Originally developed at FaceBook, it was originally a Python API to the (no longer developed) Lua 'Torch' library. It has gained in popularity to compete with TensorFlow/Keras. \n",
    "\n",
    "Similarly based on a tensor representation of a computational network graph, a major difference is the architecture is inherently _dynamic_, _i.e._ the graph architecture may be altered during training (contrast this with the Keras computational graph, which is statically compiled). This can be great for Recurrent Neural Net (RNN) architectures that have a variable output shape (_e.g._ text generation, where word lengths vary). However, this does mean that the library is lower level than that for Keras.\n",
    "\n",
    "One objection to PyTorch, compared to TensorFlow/Keras, was the relative difficulty of deploying the former in production. However, this is currently being improved upon. There is much overlap in functionality and performance between the two frameworks.\n",
    "\n",
    "The PyTorch project may be found here: https://pytorch.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appropriate applications of Artificial Neural Networks\n",
    "\n",
    "Because ANNs are 'universal approximators,' as well as based on a large number of small, simple, units, they are great where:\n",
    " * The relationships between variables are poorly understood or analytically complex\n",
    " * There is a lot of data\n",
    " \n",
    "They are not so great because:\n",
    " * Principal features are not explicitly apparent; decisions are opaque for deep networks\n",
    " * They can be slow to train, requiring a number of epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional resources\n",
    "\n",
    "\n",
    "**Websites:** \n",
    "\n",
    "  * Michael A. Neilsen's _Neural Nets and Deep Learning_: http://neuralnetworksanddeeplearning.com/\n",
    "  * Ian Goodfellow, Yoshua Bengio and Aaron Courville's _Deep Learning_: http://www.deeplearningbook.org\n",
    "\n",
    "**YouTube channels:**\n",
    "\n",
    " * Andrew Ng's _Machine Learning_: https://www.youtube.com/watch?v=PPLop4L2eGk&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN\n",
    " * Sentdex's _Practical Machine Learning with Python_: https://www.youtube.com/watch?v=OGxgnH8y2NM&index=1&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v\n",
    "\n",
    "**Platforms:** \n",
    "\n",
    " * Kaggle: https://www.kaggle.com/\n",
    " * Coursera (Andrew Ng again): https://www.coursera.org/learn/machine-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This was a very brief introduction to the field of artificial neural networks (ANNs) and Deep Learning! \n",
    "\n",
    "We have examined the theoretical justification for (ANNs), demonstrating that they are great 'universal approximators'. We also covered their use-cases and some of their pit-falls.\n",
    "\n",
    "We also had a brief introduction to TensorFlow and Keras. We built a feed-forward network (a Multi-Layer Perceptron; MLP) to approximate complex functions. We 'tweaked' this model, improving the output, and evaluated its performance. In order to determine this, we also covered the concepts of appropriate activation functions, optimizers and cost functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
